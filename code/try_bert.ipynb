{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-UI0--XWgud",
        "outputId": "c9240809-5a7f-402c-a4bb-ed4426dc6819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (4.27.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/zhuyuezx/miniconda3/envs/csc311/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GCfXfCfNWguf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3362
        },
        "id": "bQRqtuIbkkU0",
        "outputId": "c1029ed5-4a40-475e-ff8f-80bc9ebac2d9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>Array</th>\n",
              "      <th>Dynamic Programming</th>\n",
              "      <th>String</th>\n",
              "      <th>Math</th>\n",
              "      <th>Tree</th>\n",
              "      <th>Depth-first Search</th>\n",
              "      <th>Greedy</th>\n",
              "      <th>Hash Table</th>\n",
              "      <th>Binary Search</th>\n",
              "      <th>...</th>\n",
              "      <th>Random</th>\n",
              "      <th>Dequeue</th>\n",
              "      <th>Binary Search Tree</th>\n",
              "      <th>Suffix Array</th>\n",
              "      <th>Rolling Hash</th>\n",
              "      <th>Reservoir Sampling</th>\n",
              "      <th>Rejection Sampling</th>\n",
              "      <th>Memoization</th>\n",
              "      <th>OOP</th>\n",
              "      <th>Meet in the Middle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>An array is monotonic if it is either monotone...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>There is an authentication system that works w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>Given the `root` of a binary tree, return the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>The set `[1, 2, 3, ..., n]` contains a total o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Given an integer array `nums`, find the contig...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>Given a characters array `tasks`, representing...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Given a sorted array nums, remove the duplicat...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>You are given an integer array `nums` with no ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>Given a binary tree `root`, a node X in the tr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1343</th>\n",
              "      <td>Given a string `s` of lower and upper case Eng...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            description  Array  \\\n",
              "802   An array is monotonic if it is either monotone...      1   \n",
              "1547  There is an authentication system that works w...      0   \n",
              "467   Given the `root` of a binary tree, return the ...      0   \n",
              "59    The set `[1, 2, 3, ..., n]` contains a total o...      0   \n",
              "52    Given an integer array `nums`, find the contig...      1   \n",
              "543   Given a characters array `tasks`, representing...      1   \n",
              "25    Given a sorted array nums, remove the duplicat...      1   \n",
              "574   You are given an integer array `nums` with no ...      0   \n",
              "1264  Given a binary tree `root`, a node X in the tr...      0   \n",
              "1343  Given a string `s` of lower and upper case Eng...      0   \n",
              "\n",
              "      Dynamic Programming  String  Math  Tree  Depth-first Search  Greedy  \\\n",
              "802                     0       0     0     0                   0       0   \n",
              "1547                    0       0     0     0                   0       0   \n",
              "467                     0       0     0     1                   1       0   \n",
              "59                      0       0     1     0                   0       0   \n",
              "52                      1       0     0     0                   0       0   \n",
              "543                     0       0     0     0                   0       1   \n",
              "25                      0       0     0     0                   0       0   \n",
              "574                     0       0     0     1                   0       0   \n",
              "1264                    0       0     0     1                   1       0   \n",
              "1343                    0       1     0     0                   0       0   \n",
              "\n",
              "      Hash Table  Binary Search  ...  Random  Dequeue  Binary Search Tree  \\\n",
              "802            0              0  ...       0        0                   0   \n",
              "1547           1              0  ...       0        0                   0   \n",
              "467            0              0  ...       0        0                   0   \n",
              "59             0              0  ...       0        0                   0   \n",
              "52             0              0  ...       0        0                   0   \n",
              "543            0              0  ...       0        0                   0   \n",
              "25             0              0  ...       0        0                   0   \n",
              "574            0              0  ...       0        0                   0   \n",
              "1264           0              0  ...       0        0                   0   \n",
              "1343           0              0  ...       0        0                   0   \n",
              "\n",
              "      Suffix Array  Rolling Hash  Reservoir Sampling  Rejection Sampling  \\\n",
              "802              0             0                   0                   0   \n",
              "1547             0             0                   0                   0   \n",
              "467              0             0                   0                   0   \n",
              "59               0             0                   0                   0   \n",
              "52               0             0                   0                   0   \n",
              "543              0             0                   0                   0   \n",
              "25               0             0                   0                   0   \n",
              "574              0             0                   0                   0   \n",
              "1264             0             0                   0                   0   \n",
              "1343             0             0                   0                   0   \n",
              "\n",
              "      Memoization  OOP  Meet in the Middle  \n",
              "802             0    0                   0  \n",
              "1547            0    0                   0  \n",
              "467             0    0                   0  \n",
              "59              0    0                   0  \n",
              "52              0    0                   0  \n",
              "543             0    0                   0  \n",
              "25              0    0                   0  \n",
              "574             0    0                   0  \n",
              "1264            0    0                   0  \n",
              "1343            0    0                   0  \n",
              "\n",
              "[10 rows x 44 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"../data/leetcode.csv\")\n",
        "\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3orkdteEWsdy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ethan/Documents/UofT/Junior/2023Winter/CSC413/CSC413-Project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjhTQJgQfW2k",
        "outputId": "7d559a50-7c49-4933-f792-d9668558f6d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['given', 'an', 'array', 'of', 'integers', '`', 'nu', '##ms', '`', 'and', 'an', 'integer', '`', 'target', '`', ',', 'return', 'indices', 'of', 'the', 'two', 'numbers', 'such', 'that', 'they', 'add', 'up', 'to', '`', 'target', '`', '.', 'you', 'may', 'assume', 'that', 'each', 'input', 'would', 'have', 'exactly', 'one', 'solution', ',', 'and', 'you', 'may', 'not', 'use', 'the', 'same', 'element', 'twice', '.', 'you', 'can', 'return', 'the', 'answer', 'in', 'any', 'order', '.', 'example', '1', ':', 'input', ':', 'nu', '##ms', '=', '[', '2', ',', '7', ',', '11', ',', '15', ']', ',', 'target', '=', '9', 'output', ':', '[', '0', ',', '1', ']', 'output', ':', 'because', 'nu', '##ms', '[', '0', ']', '+', 'nu', '##ms', '[', '1', ']', '=', '=', '9', ',', 'we', 'return', '[', '0', ',', '1', ']', '.', 'example', '2', ':', 'input', ':', 'nu', '##ms', '=', '[', '3', ',', '2', ',', '4', ']', ',', 'target', '=', '6', 'output', ':', '[', '1', ',', '2', ']', 'example', '3', ':', 'input', ':', 'nu', '##ms', '=', '[', '3', ',', '3', ']', ',', 'target', '=', '6', 'output', ':', '[', '0', ',', '1', ']', 'constraints', ':', '`', '2', '<', '=', 'nu', '##ms', '.', 'length', '<', '=', '103', '`', '`', '-', '109', '<', '=', 'nu', '##ms', '[', 'i', ']', '<', '=', '109', '`', '`', '-', '109', '<', '=', 'target', '<', '=', '109', '`', 'only', 'one', 'valid', 'answer', 'exists', '.']\n",
            "tensor([[  101,  2445,  2019,  9140,  1997, 24028,  1036, 16371,  5244,  1036,\n",
            "          1998,  2019, 16109,  1036,  4539,  1036,  1010,  2709, 29299,  1997,\n",
            "          1996,  2048,  3616,  2107,  2008,  2027,  5587,  2039,  2000,  1036,\n",
            "          4539,  1036,  1012,  2017,  2089,  7868,  2008,  2169,  7953,  2052,\n",
            "          2031,  3599,  2028,  5576,  1010,  1998,  2017,  2089,  2025,  2224,\n",
            "          1996,  2168,  5783,  3807,  1012,  2017,  2064,  2709,  1996,  3437,\n",
            "          1999,  2151,  2344,  1012,  2742,  1015,  1024,  7953,  1024, 16371,\n",
            "          5244,  1027,  1031,  1016,  1010,  1021,  1010,  2340,  1010,  2321,\n",
            "          1033,  1010,  4539,  1027,  1023,  6434,  1024,  1031,  1014,  1010,\n",
            "          1015,  1033,  6434,  1024,  2138, 16371,  5244,  1031,  1014,  1033,\n",
            "          1009, 16371,  5244,  1031,  1015,  1033,  1027,  1027,  1023,  1010,\n",
            "          2057,  2709,  1031,  1014,  1010,  1015,  1033,  1012,  2742,  1016,\n",
            "          1024,  7953,  1024, 16371,  5244,  1027,  1031,  1017,  1010,  1016,\n",
            "          1010,  1018,  1033,  1010,  4539,  1027,  1020,  6434,  1024,  1031,\n",
            "          1015,  1010,  1016,  1033,  2742,  1017,  1024,  7953,  1024, 16371,\n",
            "          5244,  1027,  1031,  1017,  1010,  1017,  1033,  1010,  4539,  1027,\n",
            "          1020,  6434,  1024,  1031,  1014,  1010,  1015,  1033, 14679,  1024,\n",
            "          1036,  1016,  1026,  1027, 16371,  5244,  1012,  3091,  1026,  1027,\n",
            "          9800,  1036,  1036,  1011, 11518,  1026,  1027, 16371,  5244,  1031,\n",
            "          1045,  1033,  1026,  1027, 11518,  1036,  1036,  1011, 11518,  1026,\n",
            "          1027,  4539,  1026,  1027, 11518,  1036,  2069,  2028,  9398,  3437,\n",
            "          6526,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "test_input = df.iloc[0]['description']\n",
        "# print(test_input)\n",
        "test_tokens = tokenizer.tokenize(test_input)\n",
        "test_ids = tokenizer.encode(test_input, add_special_tokens=True, return_tensors='pt')\n",
        "print(test_tokens)\n",
        "print(test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data(df):\n",
        "    texts = df['description'].values\n",
        "    lables = df.iloc[:, 1:].values\n",
        "    # convert texts to list of strings\n",
        "    texts = [str(text) for text in texts]\n",
        "\n",
        "    # do train val test split\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, lables, test_size=0.2, random_state=SEED)\n",
        "    val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=SEED)\n",
        "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts, val_texts, test_texts, train_labels, val_labels, test_labels = split_data(df)\n",
        "num_topics = len(train_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class my_bert(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(768, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, num_topics),\n",
        "        )\n",
        "        self.activation = torch.nn.Sigmoid()\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "    \n",
        "    def forward(self, labels = None, **kwargs):\n",
        "        outputs = super().forward(**kwargs)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
        "        probs = self.activation(logits)\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(probs, labels)\n",
        "            return loss, probs\n",
        "        else:\n",
        "            return (probs,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels, dtype=torch.float32))\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels, dtype=torch.float32))\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(batch_size = 16, num_epochs = 10):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * len(input_ids)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                loss, probs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                val_preds.append(probs.cpu().numpy())\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_preds = np.vstack(val_preds)\n",
        "        \n",
        "        val_labels = val_labels.astype(int)\n",
        "        val_preds = (val_preds > 0.5).astype(int)\n",
        "        accuracy = np.mean(np.sum(val_labels == val_preds, axis=1) == num_topics)\n",
        "        f1_score = f1_score(val_labels, val_preds, average='macro')\n",
        "        print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Accuracy: {accuracy:.4f} | F1 Score: {f1_score:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m probs \u001b[39m=\u001b[39m activation(logits)\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(probs, labels)\n\u001b[0;32m---> 21\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(input_ids)\n",
            "File \u001b[0;32m~/Documents/UofT/Junior/2023Winter/CSC413/CSC413-Project/.venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/UofT/Junior/2023Winter/CSC413/CSC413-Project/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "csc311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
